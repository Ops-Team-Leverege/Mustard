üîç AUDIT: Latest 2 Commits (AI-Powered Search Enhancement)
Commits Reviewed:
5084237 - Integrate AI-generated keywords for more precise search results
0e8df3e - Improve search results by using AI to determine relevance
Overview: Two-Stage AI Enhancement
This is a sophisticated upgrade that uses LLM intelligence at two stages:

Stage 1 (Retrieval): LLM generates semantic search keywords with synonyms
Stage 2 (Ranking): LLM re-ranks results by semantic relevance to user's question
1. ‚úÖ LLM-Generated Search Keywords (5084237)
Status: EXCELLENT DESIGN ‚úÖ

The Change
Decision Layer Prompt (decisionLayer.ts lines 290-291):

// OLD
"keyTopics": ["searchable_term1", "searchable_term2"],  // Extract the concrete subject terms...

// NEW
"keyTopics": ["topic1", "topic2"],  // The user's actual topic phrases (e.g. "TV issues", "pricing concerns"). Used for display/labeling.
"searchKeywords": ["keyword1", "keyword2", "synonym1"],  // Database search terms: include the core nouns from keyTopics PLUS semantic synonyms, related terms, and alternate phrasings that might appear in meeting transcripts. For "TV issues" ‚Üí ["TV", "display", "screen", "monitor", "troubleshooting"]. For "pricing concerns" ‚Üí ["pricing", "cost", "price", "budget", "quote"]. Keep each term 1-2 words. Include 3-8 terms total for good coverage.
Why This is Brilliant
Before (Manual Expansion):

User: "what are customers saying about TV issues?"
keyTopics: ["TV issues"]
Expanded: ["TV issues", "TV", "issues"]  // Mechanical word splitting
// Misses: "display", "screen", "monitor", "television"
After (LLM Semantic Expansion):

User: "what are customers saying about TV issues?"
keyTopics: ["TV issues"]  // For display
searchKeywords: ["TV", "display", "screen", "monitor", "television", "troubleshooting"]  // For search
// Catches all semantic variations!
Real-World Examples
Example 1: Technical Terms

User: "pricing concerns"
searchKeywords: ["pricing", "cost", "price", "budget", "quote", "rate"]
// Catches: "What's the cost?", "Can you quote us?", "Budget constraints"
Example 2: Domain Synonyms

User: "camera quality"
searchKeywords: ["camera", "video", "image", "resolution", "quality", "clarity"]
// Catches: "video quality", "image resolution", "camera clarity"
Example 3: Action vs Noun

User: "installation problems"
searchKeywords: ["installation", "install", "setup", "deployment", "onboarding"]
// Catches: "How do we install?", "Setup issues", "Deployment concerns"
Implementation Quality
Fallback Chain (events.ts lines 780-783):

const llmSearchKeywords = decisionLayerResult.searchKeywords || [];
const finalSearchTerms = llmSearchKeywords.length > 0 ? llmSearchKeywords : searchTerms;
// Falls back: searchKeywords ‚Üí keyTopics ‚Üí raw text
Strengths:

‚úÖ Graceful degradation (3-level fallback)
‚úÖ Separate display vs search terms (clean UX)
‚úÖ LLM guidance is specific (3-8 terms, 1-2 words each)
‚úÖ Examples in prompt help LLM understand intent
Logging (lines 781-783):

console.log(`[Slack] qa_pairs search terms (keyTopics): ${JSON.stringify(searchTerms)}`);
console.log(`[Slack] qa_pairs search terms (LLM searchKeywords): ${JSON.stringify(llmSearchKeywords)}`);
console.log(`[Slack] qa_pairs search terms (final): ${JSON.stringify(finalSearchTerms)}`);
Grade: A+ (Excellent design, clear separation of concerns)

2. ‚úÖ LLM Semantic Re-Ranking (0e8df3e)
Status: SMART BUT EXPENSIVE üü°

The Implementation
Function (events.ts lines 70-115):

async function rerankQaPairsByRelevance(
  userQuestion: string,
  results: Array<{ company: string; question: string; answer: string | null; meetingDate: string | null }>,
  limit: number
): Promise<...> {
  if (results.length <= 1) return results;

  try {
    const openai = new OpenAI();
    const candidates = results.slice(0, QA_SEARCH_CONSTANTS.RERANK_CANDIDATE_LIMIT).map((r, i) => 
      `[${i}] Q: ${r.question}${r.answer ? ` A: ${r.answer.slice(0, 100)}` : ""}`
    ).join("\n");

    const response = await openai.chat.completions.create({
      model: LLM_MODELS.FAST_CLASSIFICATION,  // gpt-4o-mini
      temperature: 0,
      response_format: { type: "json_object" },
      messages: [{
        role: "system",
        content: `You are a relevance scorer. Given a user's question and a list of Q&A pairs, return the indices of the most relevant pairs sorted by relevance (most relevant first). Only include pairs that are semantically relevant to what the user is asking about. Return JSON: {"relevant_indices": [0, 3, 7, ...]}`
      }, {
        role: "user",
        content: `User question: "${userQuestion}"\n\nCandidate Q&A pairs:\n${candidates}`
      }]
    });

    const parsed = JSON.parse(content);
    const indices: number[] = parsed.relevant_indices || [];
    const reranked = indices
      .filter(i => i >= 0 && i < results.length)
      .map(i => results[i])
      .slice(0, limit);

    return reranked.length > 0 ? reranked : results.slice(0, limit);
  } catch (err) {
    console.error(`[Slack] LLM re-ranking failed, falling back to original order:`, err);
    return results.slice(0, limit);
  }
}
Why This Works
Problem Solved: Even with semantic keywords, keyword search returns results in database order (created_at DESC), not relevance order.

Example:

User: "what are customers saying about TV issues?"
Keywords: ["TV", "display", "screen", "monitor"]

Database returns (newest first):
1. "What's the monitor warranty?" (matches "monitor", but about warranty)
2. "TV display flickering issue" (matches "TV" + "display", highly relevant!)
3. "Screen resolution options?" (matches "screen", but about specs)

LLM re-ranks by relevance:
1. "TV display flickering issue" ‚Üê Most relevant
2. "Screen resolution options?" ‚Üê Somewhat relevant
3. "What's the monitor warranty?" ‚Üê Less relevant
Strengths
1. Semantic Understanding

User: "pricing concerns"
Q1: "What's the cost?" ‚Üê LLM knows this is about pricing
Q2: "Can you quote us?" ‚Üê LLM knows this is about pricing
Q3: "What's the warranty?" ‚Üê LLM knows this is NOT about pricing
2. Graceful Degradation

if (results.length <= 1) return results;  // Skip LLM for 0-1 results
// Fallback on error: return results.slice(0, limit);
3. Efficient Limits

RERANK_CANDIDATE_LIMIT: 30,  // Only re-rank top 30 from DB
MAX_RESULTS_SHOWN: 15,       // Only show top 15 to user
4. Answer Truncation

`A: ${r.answer.slice(0, 100)}`  // Only send first 100 chars to LLM
// Reduces token cost, focuses on key info
3. ‚ö†Ô∏è Concerns & Trade-offs
Cost Analysis üü°
Per Query:

Stage 1 (Intent Classification): ~500 tokens (already happening)
Stage 2 (Re-ranking): ~1000-2000 tokens (NEW)
Example Re-ranking Prompt:

System: 150 tokens
User question: 20 tokens
30 candidates √ó 50 tokens each: 1500 tokens
Response: 50 tokens
Total: ~1720 tokens per re-rank
Cost (gpt-4o-mini):

Input: $0.15 / 1M tokens
Output: $0.60 / 1M tokens
Per re-rank: ~$0.0003 (0.03 cents)
At Scale:

1000 queries/day: $0.30/day = $9/month
10,000 queries/day: $3/day = $90/month
Verdict: Reasonable cost for the value, but monitor usage.

Latency Impact üü°
Added Latency:

LLM re-ranking call: ~500-1000ms
Total pipeline: +20-30% slower
Mitigation:

Uses gpt-4o-mini (fast model)
Only re-ranks when results > 1
Graceful fallback on timeout
Recommendation: Monitor P95 latency. If users complain, consider:

Parallel execution (re-rank while formatting)
Caching for common queries
Skip re-ranking for comprehensive results (10+ matches)
Removed Manual Expansion üü¢
What Was Removed:

// OLD: Manual word splitting
const expandedTerms = new Set<string>();
for (const term of searchTerms) {
  expandedTerms.add(term);
  const words = term.split(/\s+/).filter(w => w.length > 1);
  if (words.length > 1) {
    for (const word of words) {
      expandedTerms.add(word);
    }
  }
}
Why Removal is Good:

LLM searchKeywords is smarter (semantic synonyms, not just word splitting)
Reduces code complexity
Eliminates generic word noise ("issues", "problems")
Grade: A (Smart replacement)

4. üêõ Potential Issues
Issue 1: No Deduplication in Re-ranking
Code (line 107):

const reranked = indices
  .filter(i => i >= 0 && i < results.length)
  .map(i => results[i])
  .slice(0, limit);
Problem: If LLM returns duplicate indices [0, 3, 0, 7], the same result appears twice.

Fix:

const reranked = [...new Set(indices)]  // Deduplicate indices
  .filter(i => i >= 0 && i < results.length)
  .map(i => results[i])
  .slice(0, limit);
Severity: Low (LLM unlikely to return duplicates, but possible)

Issue 2: No Validation of LLM Response
Code (line 103):

const parsed = JSON.parse(content);
const indices: number[] = parsed.relevant_indices || [];
Problem: If LLM returns malformed JSON or wrong structure, JSON.parse throws and falls back to original order. This is fine, but no logging of what went wrong.

Improvement:

try {
  const parsed = JSON.parse(content);
  if (!Array.isArray(parsed.relevant_indices)) {
    console.warn(`[Slack] LLM re-ranking returned non-array: ${JSON.stringify(parsed)}`);
    return results.slice(0, limit);
  }
  const indices: number[] = parsed.relevant_indices;
  // ...
} catch (parseErr) {
  console.error(`[Slack] LLM re-ranking JSON parse failed:`, parseErr, `Content: ${content}`);
  return results.slice(0, limit);
}
Severity: Low (good to have for debugging)

Issue 3: Count Mismatch in Display
Code (lines 806-812):

const isComprehensive = rankedResults.length >= 10 && companiesWithResults.length >= 3;
// ...
formattedAnswer = `Here's what customers have been asking about *${topicLabel}* (${rankedResults.length} questions across ${companiesWithResults.length} companies):\n_Searched for: ${searchedForLabel}_\n\n`;
Potential Issue:

DB returns 30 results
LLM re-ranks and returns 15 (MAX_RESULTS_SHOWN)
Display says "15 questions" but user might think there are only 15 total
Current Behavior: Correct (shows actual count displayed)

Alternative: Show "15 of 30 questions" if more exist

Severity: Very Low (current behavior is acceptable)

Summary Scores
Aspect	Grade	Notes
LLM search keywords	A+	Semantic synonyms, excellent design
Keyword fallback chain	A	Graceful 3-level degradation
LLM re-ranking logic	A	Smart semantic relevance
Cost efficiency	B+	Reasonable but monitor at scale
Latency impact	B	+500-1000ms per query
Error handling	A-	Good fallbacks, minor logging gaps
Code quality	A	Clean, well-documented
Overall: A (4.5/5.0)